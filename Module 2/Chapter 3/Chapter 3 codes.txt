# 1 Selecting  column(s)
import pandas as pd
data=pd.read_csv('E:/Personal/Learning/Predictive Modeling Book/Book Datasets/Customer Churn Model.txt')
account_length=data['Account Length']
account_length.head()

subdata=data[['Account Length','VMail Message','Day Calls']]
subdata.head()


# 2 Selecting/Ignoring particular columns
wanted_columns=['Account Length','VMail Message','Day Calls']
subdata=data[wanted]
subdata.head()

wanted=['Account Length','VMail Message','Day Calls']
column_list=data.columns.values.tolist()
sublist=[x for x in column_list if x not in wanted]
subdata=data[sublist]
subdata.head()


# 3 Selecting rows with row numbers and conditional filters
data[1:50]
data[25:75]
data[:50] 
data[51:]

data1=data[data['Total Mins']>500]
data1.shape

data1=data[data['State']==’VA’]
data1.shape

data1=data[(data['Total Mins']>500) & (data['State']=='VA')]
data1.shape

data1=data[(data['Total Mins']>500) | (data['State']=='VA')]
data1.shape


#4 Selecting combination of rows and columns
subdata_first_50=data[['Account Length','VMail Message','Day Calls']][1:50]
subdata_first_50

data.ix[1:100,1:6]
data.ix[:,1:6]            
data.ix[1:100,:]
data.ix[1:100,[2,5,7]]
data.ix[[1,2,5],[2,5,7]]
data.ix[[1,2,5],['Area Code','VMail Plan','Day Mins']]


# 5 Creating new columns
data['Total Mins']=data['Day Mins']+data['Eve Mins']+data['Night Mins']
data['Total Mins'].head() 


# 6 Generating Random Numbers
import numpy as np
np.random.randint(1,100)

import numpy as np
np.random.random()

def randint_range(n,a,b):
    x=[]
    for i in range(n):
        x.append(np.random.randint(a,b))
    return x

import random
for i in range(3):
    print random.randrange(0,100,5) 

a=range(100)
np.random.shuffle(a)

import pandas as pd
data=pd.read_csv('E:/Personal/Learning/Predictive Modeling Book/Book Datasets/Customer Churn Model.txt')
column_list=data.columns.values.tolist()
np.random.choice(column_list)


# 7 Seeding a Random Number
np.random.seed(1)
for i in range(5):
    print np.random.random()


# 8 Histogram of Uniformly and Normally distributed random numbers

import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
a=np.random.uniform(1,100,100)
b=range(1,101)
plt.hist(a)

import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
a=np.random.randn(100)
b=range(1,101)
plt.plot(b,a)

import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
a=np.random.randn(100000)
b=range(1,101)
plt.hist(a)


#9 Monte Carlo Simulations to find value of pi

def pi_run(nums,loops):
    pi_avg=0
    pi_value_list=[]
    for i in range(loops):
        value=0
        x=np.random.uniform(0,1,nums).tolist()
        y=np.random.uniform(0,1,nums).tolist()
        for j in range(nums):
            z=np.sqrt(x[j]*x[j]+y[j]*y[j])
            if z<=1:
                value+=1
        float_value=float(value)
        pi_value=float_value*4/nums
        pi_value_list.append(pi_value)
        pi_avg+=pi_value
    
    pi=pi_avg/loops
    ind=range(1,loops+1)
    fig=plt.plot(ind,pi_value_list)
    return (pi,fig)


# 10 Generating a dummy data frame

import pandas as pd
data = pd.read_csv('E:/Personal/Learning/Datasets/Book/Customer Churn Model.txt') 
column_list=data.columns.values.tolist()
a=len(column_list)
d=pd.DataFrame({'Column_Name':column_list,'A':np.random.randn(a),'B':2.5*np.random.randn(a)+1.5})
d

import pandas as pd
d=pd.DataFrame({'A':np.random.randn(10),'B':2.5*np.random.randn(10)+1.5},index=range(10,20))
d

# 11 Generating a dummy data frame to illustrate groupby method

import numpy as np
import pandas as pd
a=['Male','Female']
b=['Rich','Poor','Middle Class']
gender=[]
seb=[]
for i in range(1,101):
    gender.append(np.random.choice(a))
    seb.append(np.random.choice(b))
height=30*np.random.randn(100)+155
weight=20*np.random.randn(100)+60
age=10*np.random.randn(100)+35
income=1500*np.random.randn(100)+15000

df=pd.DataFrame({'Gender':gender,'Height':height,'Weight':weight,'Age':age,'Income':income,'Socio-Eco':seb})
df.head()

# 12 Print group names and groups

grouped=df.groupby('Gender')
for names,groups in grouped:
    print names
    print groups

grouped=df.groupby(['Gender','Socio-Eco'])
for names,groups in grouped:
    print names
    print groups


# 13 Aggregation

grouped=df.groupby(['Gender','Socio-Eco'])
grouped.sum()

grouped=df.groupby(['Gender','Socio-Eco'])
grouped.size()

grouped=df.groupby(['Gender','Socio-Eco'])
grouped.describe()

grouped=df.groupby(['Gender','Socio-Eco'])
grouped.aggregate({'Income':np.sum,'Age':np.mean,'Height':np.std})

grouped=df.groupby(['Gender','Socio-Eco'])
grouped.aggregate({'Age':np.mean,'Height':lambda x:np.mean(x)/np.std(x)})

grouped.aggregate([np.sum, np.mean, np.std])

# 14 Filtering

grouped['Age'].filter(lambda x:x.sum()>700)


# 15 Transformation

zscore = lambda x: (x - x.mean()) / x.std()
grouped.transform(zscore)

f = lambda x: x.fillna(x.mean()
grouped.transform(f)

# 16 Miscellaneous Operations

grouped.head(1)
grouped.tail(1)

grouped=df.groupby('Gender')
grouped.nth(1)

df1=df.sort(['Age','Income'])
grouped=df1.groupby('Gender')
grouped.head(1)


# 17 Random Sampling 

Method 1
import pandas as pd
data = pd.read_csv('E:/Personal/Learning/Datasets/Book/Customer Churn Model.txt')
len(data)
a=np.random.randn(len(data))
check=a<0.8
training=data[check]
testing=data[~check]

Method 2
from sklearn.cross_validation import train_test_split
train, test = train_test_split(data, test_size = 0.2)

Method 3
import numpy as np
with open('E:/Personal/Learning/Datasets/Book/Customer Churn Model.txt','rb') as f:
    data=f.read().split('\n')
np.random.shuffle(data)
train_data = data[:3*len(data)/4]
test_data = data[len(data)/4:]


# 18 Concatenating Datasets

import pandas as pd
data1=pd.read_csv('E:/Personal/Learning/Predictive Modeling Book/Book Datasets/Merge and Join/winequality-red.csv',sep=';')
data1.head()

import pandas as pd
data2=pd.read_csv('E:/Personal/Learning/Predictive Modeling Book/Book Datasets/Merge and Join/winequality-white.csv',sep=';')
data2.head()

wine_total=pd.concat([data1,data2],axis=0)

data1_head=data1.head(50)
data1_middle=data1[500:550]
data1_tail=data.tail(50)
wine_scramble=pd.concat([data1_middle,data1_head,data1_tail],axis=0)
wine_scramble

data=pd.read_csv('E:/Personal/Learning/Predictive Modeling Book/Book Datasets/Merge and Join/lotofdata/001.csv')
data.head()

import pandas as pd
filepath='E:/Personal/Learning/Predictive Modeling Book/Book Datasets/Merge and Join/lotofdata'
data_final=pd.read_csv('E:/Personal/Learning/Predictive Modeling Book/Book Datasets/Merge and Join/lotofdata/001.csv')
for i in range(1,333):
    if i<10:
        filename='0'+'0'+str(i)+'.csv'
    if 10<=i<100:
        filename='0'+str(i)+'.csv'
    if i>=100:
        filename=str(i)+'.csv'
        
    file=filepath+'/'+filename
    data=pd.read_csv(file)
    
    data_final=pd.concat([data_final,data],axis=0)


import pandas as pd
filepath='E:/Personal/Learning/Predictive Modeling Book/Book Datasets/Merge and Join/lotofdata'
data_final=pd.read_csv('E:/Personal/Learning/Predictive Modeling Book/Book Datasets/Merge and Join/lotofdata/001.csv')
data_final_size=len(data_final)
for i in range(1,333):
    if i<10:
        filename='0'+'0'+str(i)+'.csv'
    if 10<=i<100:
        filename='0'+str(i)+'.csv'
    if i>=100:
        filename=str(i)+'.csv'
        
    file=filepath+'/'+filename
    data=pd.read_csv(file)
    data_final_size+=len(data)
    
    
    data_final=pd.concat([data_final,data],axis=0)
print data_final_size


# 19 Merging/Joining Datasets

import pandas as pd
data_main=pd.read_csv('E:/Personal/Learning/Predictive Modeling Book/Book Datasets/Merge and Join/Medals/Medals.csv')
data_main.head()

a=data_main['Athlete'].unique().tolist()
len(a)

country_map=pd.read_csv('E:/Personal/Learning/Predictive Modeling Book/Book Datasets/Merge and Join/Medals/Athelete_Country_Map.csv')
country_map.head()

sports_map=pd.read_csv('E:/Personal/Learning/Predictive Modeling Book/Book Datasets/Merge and Join/Medals/Athelete_Sports_Map.csv')
sports_map.head()

sports_map[(sports_map['Athlete']=='Chen Jing') | (sports_map['Athlete']=='Richard Thompson') | (sports_map['Athlete']=='Matt Ryan')]

import pandas as pd
merged=pd.merge(left=data_main,right=country_map,left_on='Athlete',right_on='Athlete')
merged.head()

merged[merged['Athlete']=='Aleksandar Ciric']

country_map_dp=country_map.drop_duplicates(subset='Athlete')

merged_dp=pd.merge(left=data_main,right=country_map_dp,left_on='Athlete',right_on='Athlete')
len(merged_dp)

sports_map_dp=sports_map.drop_duplicates(subset='Athlete')
len(sports_map_dp)

merged_final=pd.merge(left=merged_dp,right=sports_map_dp,left_on='Athlete',right_on='Athlete')
merged_final.head()

country_map_dlt=country_map_dp[(country_map_dp['Athlete']<>'Michael Phelps') & (country_map_dp['Athlete']<>'Natalie Coughlin') & (country_map_dp['Athlete']<>'Chen Jing')
                    & (country_map_dp['Athlete']<>'Richard Thompson') & (country_map_dp['Athlete']<>'Matt Ryan')]
len(country_map_dlt)

sports_map_dlt=sports_map_dp[(sports_map_dp['Athlete']<>'Michael Phelps') & (sports_map_dp['Athlete']<>'Natalie Coughlin') & (sports_map_dp['Athlete']<>'Chen Jing')
                    & (sports_map_dp['Athlete']<>'Richard Thompson') & (sports_map_dp['Athlete']<>'Matt Ryan')]
len(sports_map_dlt)

data_main_dlt=data_main[(data_main['Athlete']<>'Michael Phelps') & (data_main['Athlete']<>'Natalie Coughlin') & (data_main['Athlete']<>'Chen Jing')
                    & (data_main['Athlete']<>'Richard Thompson') & (data_main['Athlete']<>'Matt Ryan')]
len(data_main_dlt)

Inner Join
merged_inner=pd.merge(left=data_main,right=country_map_dlt,how='inner',left_on='Athlete',right_on='Athlete')
len(merged_inner)

Left Join
merged_left=pd.merge(left=data_main,right=country_map_dlt,how='left',left_on='Athlete',right_on='Athlete')
len(merged_left)

merged_left_slt=merged_left[merged_left['Athlete']=='Michael Phelps']
merged_left_slt

Right Join
merged_right=pd.merge(left=data_main_dlt,right=country_map_dp,how='right',left_on='Athlete',right_on='Athlete')
len(merged_right)








